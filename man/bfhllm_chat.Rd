% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/chat.R
\name{bfhllm_chat}
\alias{bfhllm_chat}
\title{LLM Chat Interface}
\usage{
bfhllm_chat(
  prompt,
  model = NULL,
  provider = "gemini",
  timeout = NULL,
  max_chars = NULL,
  cache = NULL,
  validate = TRUE
)
}
\arguments{
\item{prompt}{Character string, prompt to send to LLM}

\item{model}{Character string, model identifier (default: from config)}

\item{provider}{Character string, provider name (default: "gemini")}

\item{timeout}{Numeric, timeout in seconds (default: from config)}

\item{max_chars}{Integer, maximum response length (default: from config)}

\item{cache}{Cache object from \code{bfhllm_cache_create()} or \code{bfhllm_cache_shiny()}
(optional). If provided, responses are cached to reduce API calls.}

\item{validate}{Logical, whether to validate and sanitize response (default: TRUE)}
}
\value{
Character string with LLM response, or NULL on error
}
\description{
Generic chat function with provider abstraction, caching, circuit breaker
protection, and response validation.
}
\details{
\strong{Features:}
\itemize{
\item Provider abstraction (currently Gemini, extensible to others)
\item Circuit breaker protection (opens after threshold failures)
\item Optional caching (reduces API calls and costs)
\item Response validation (HTML removal, markdown balancing)
\item Timeout handling
}

\strong{Configuration:}
Default values are read from configuration (set via \code{bfhllm_configure()} or
environment variables). Function arguments override configuration.

\strong{Caching:}
If cache is provided, responses are cached based on prompt + model hash.
Cache hits avoid API calls entirely. Use \code{bfhllm_cache_create()} for
standalone or \code{bfhllm_cache_shiny()} for Shiny apps.

\strong{Error Handling:}
Returns NULL on errors (timeout, API failure, validation failure).
Check circuit breaker status with \code{bfhllm_circuit_breaker_status()}.
}
\examples{
\dontrun{
# Basic usage
Sys.setenv(GOOGLE_API_KEY = "your_api_key")
response <- bfhllm_chat("Explain SPC in 2 sentences")
print(response)

# With caching
cache <- bfhllm_cache_create()
response1 <- bfhllm_chat("What is variation?", cache = cache)
response2 <- bfhllm_chat("What is variation?", cache = cache) # Cache hit

# Custom configuration
response <- bfhllm_chat(
  "Analyze this data pattern",
  model = "gemini-2.0-flash-exp",
  timeout = 15,
  max_chars = 500
)

# In Shiny app
server <- function(input, output, session) {
  cache <- bfhllm_cache_shiny(session)

  observeEvent(input$generate_btn, {
    response <- bfhllm_chat(
      input$prompt,
      cache = cache
    )
    output$result <- renderText(response)
  })
}
}

}
